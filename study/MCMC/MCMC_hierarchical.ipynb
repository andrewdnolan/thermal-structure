{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Modeling \n",
    "\n",
    "For a better `class` definition see [pymc3 example](https://docs.pymc.io/en/v3/pymc-examples/examples/case_studies/probabilistic_matrix_factorization.html).\n",
    "\n",
    "For a tutorial on Hierarchical check out these tutorials from the `pymc3` examples page: \n",
    "  - https://docs.pymc.io/en/v3/pymc-examples/examples/generalized_linear_models/GLM-simpsons-paradox.html\n",
    "  - https://docs.pymc.io/en/v3/pymc-examples/examples/case_studies/rugby_analytics.html\n",
    "  \n",
    "For more background about hierarchical Modeling, see these tutorials: \n",
    "  - https://twiecki.io/blog/2014/03/17/bayesian-glms-3/\n",
    "  - https://twiecki.io/blog/2017/02/08/bayesian-hierchical-non-centered/\n",
    "\n",
    "Right now, each posterior predicted per glacier is independent of one another. This isn't the most efficient, nor the most bayesian. We should implement something that's properly hierarchical, but that will mean some reparameterization: \n",
    "  - https://stackoverflow.com/questions/48184335/userwarning-pymc3-what-does-reparameterize-mean\n",
    "\n",
    "\n",
    "In terms of better priors, see: \n",
    "  - [Rounce et al. 2020](https://www-cambridge-org.proxy.lib.sfu.ca/core/services/aop-cambridge-core/content/view/61D8956E9A6C27CC1A5AEBFCDADC0432/S0022143019000911a.pdf/quantifying_parameter_uncertainty_in_a_largescale_glacier_evolution_model_using_bayesian_inference_application_to_high_mountain_asia.pdf): _Quantifying parameter uncertainty in a large-scale glacier evolution model using Bayesian\n",
    "inference: application to High Mountain Asia_\n",
    " \n",
    "From [Braithwaite 2008](https://www-cambridge-org.proxy.lib.sfu.ca/core/services/aop-cambridge-core/content/view/6C2362F61B7DE7F153247A039736D54C/S0022143000207636a.pdf/temperature-and-precipitation-climate-at-the-equilibrium-line-altitude-of-glaciers-expressed-by-the-degree-day-factor-for-melting-snow.pdf) we have sense of the prior distribution for the degree day factor for snow. Now the challenge for the hierarchical modeling is to find a way to minic said distibution as a \"parent population\" and model the deviation from that parent population for each glacier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PDD\n",
    "import glob\n",
    "import numpy as np \n",
    "import pymc3 as pm \n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Young et al. 2020\n",
    "Young2020 = xr.open_dataset(\"../MB_tune/notebooks/Young_etal_2020_Delta_T_-0.9_C.nc\")\n",
    "z_ref     = Young2020.stack(z=('x', 'y')).Elevation.mean().values\n",
    "A_mean    = Young2020.stack(z=('x', 'y')).Accumulation.mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of PDD model parameters\n",
    "const =  dict(T_m    = 0.0, \n",
    "              T_rs   = 1.0,\n",
    "              A_mean = 1.4 * 910., \n",
    "              α      = 10.5, \n",
    "              T_ma   = -6.65,\n",
    "              ΔTΔz   = 6.5E-3, \n",
    "              T_p    = 196, \n",
    "              ref_z  = 2193)\n",
    "\n",
    "# initialize the PDD melt model class\n",
    "melt_model = PDD.PDD_melt_model(**const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the individual model runs\n",
    "glacs = [('lilk-a',   '00.1'), \n",
    "         ('crmpt18-a','00.2'), \n",
    "         ('crmpt12',   -0.3)]\n",
    "\n",
    "results_fp = \"../../initialization/coarse/result/{}/nc/{}_*_{}_OFF*.nc\"\n",
    "\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for glac in glacs: \n",
    "    key, offset = glac\n",
    "    \n",
    "    src_fp = glob.glob(results_fp.format(*[key] * 2, offset))[0]\n",
    "    \n",
    "    with xr.open_dataset(src_fp) as src: \n",
    "        # correct for minimum ice thickness\n",
    "        src[\"depth\"] = xr.where(src.depth <= 10, 0, src.depth)\n",
    "        # apply sigma coordinate transform for vertical coordinate\n",
    "        src[\"Z\"]     = src.zbed + src.Z * src.height \n",
    "        # Calculate the magnitude of the velocity vectors\n",
    "        src['vel_m'] = np.sqrt(src['velocity 1']**2 + src['velocity 2']**2)    \n",
    "        \n",
    "    # Exract data of interest from NetCDF file\n",
    "    z  = src['zs'].isel(t=-1,coord_2=-1).values[np.newaxis, :]\n",
    "    MB = np.random.normal(src['zs accumulation flux 2'].isel(t=-1,coord_2=-1).values, 1e-1)[np.newaxis, :]\n",
    "        \n",
    "    dfs.append(pd.DataFrame({f'z':z[0], \"MB\": MB[0], 'key':[key]*len(z[0])}))\n",
    "        \n",
    "Elmer_Runs = pd.concat(dfs, ignore_index=True)\n",
    "Elmer_Runs[\"key_factor\"] =  pd.factorize(Elmer_Runs.key)[0]\n",
    "\n",
    "coords = {\"key\": Elmer_Runs.key.unique(),\n",
    "          \"idx\": np.arange(Elmer_Runs.shape[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \"data\", which allows hierarchical prediction\n",
    "with pm.Model(coords=coords) as model: \n",
    "    # constant model encapsulated within pandas df\n",
    "    MB = pm.Data(\"MB\", Elmer_Runs[\"MB\"], dims=\"idx\")\n",
    "    z  = pm.Data(\"z\",  Elmer_Runs[\"z\"],  dims=\"idx\")\n",
    "    g  = pm.Data(\"g\",  Elmer_Runs[\"key_factor\"],  dims=\"idx\")\n",
    "    \n",
    "# Define Priors\n",
    "with model: \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # ----> Mass balance Model (physical priors)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    #---------------------------\n",
    "    # -> global model parameters\n",
    "    #---------------------------\n",
    "    # Somewhat base of Aschwanden et al. 2019\n",
    "    C_prior = pm.TruncatedNormal(\"C\",      mu=2.0,  sigma=1.5,  lower=1)\n",
    "    # Need a reference for the accumulation grad distibution \n",
    "    grad_a  = pm.Uniform(\"grad_a\", lower=1e-4, upper=1e-2)\n",
    "    \n",
    "    #----------------------------------------\n",
    "    # -> hyperpriors (i.e. parent population)\n",
    "    #----------------------------------------\n",
    "    # Degree Day factor: hyperpriors\n",
    "    f_s_sigma  = pm.HalfCauchy('f_s_sigma', 1)\n",
    "    f_s_mu     = pm.TruncatedNormal(\"f_s_mu\", 4.1, sigma = 1.5, lower=0.0)\n",
    "    \n",
    "    f_r_sigma  = pm.HalfCauchy('f_r_sigma', 0.25)\n",
    "    f_r_mu     = pm.TruncatedNormal(\"f_r_mu\", 0.5, sigma = 0.25, lower=0.0)\n",
    "    \n",
    "    #-------------------------------------\n",
    "    # -> glacier-specific model parameters\n",
    "    #-------------------------------------\n",
    "    # Degree Day factor: Braithwaite (2008) / Rounce et al. 2020\n",
    "    f_s_offset = pm.Normal('f_s_offset', mu=0, sd=2, dims='key')\n",
    "    f_s_prior  = pm.Deterministic(\"f_s\", f_s_mu + f_s_offset * f_s_sigma)    \n",
    "    \n",
    "    \n",
    "    # Refreezing Factor: Somewhat base of Aschwanden et al. 2019\n",
    "    f_r_offset = pm.Bound(pm.Normal, lower=0.5)('f_r_offset', mu=0, sd=0.25, dims='key')\n",
    "    f_r_prior  = pm.Deterministic(\"f_r\", f_r_mu + f_r_offset * f_r_sigma) \n",
    "\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # We should probably account for the error associated with our \n",
    "    # mass balance model not able to actually fit the Young et al. results\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # ????? \n",
    "    \n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # ----> Hyperparameters (likelihood related priors)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    sigma = pm.HalfCauchy(\"sigma\", 10, )    \n",
    "\n",
    "\n",
    "\n",
    "with model: \n",
    "    mu = pm.Deterministic('mu', \n",
    "                          melt_model.tt_forward(z, f_s_prior[g], C_prior, grad_a, f_r_prior[g]), \n",
    "                          dims='idx')\n",
    "    \n",
    "# Define likelihood (function?)  \n",
    "with model: \n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    Y_obs = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=MB, dims='idx')\n",
    "    \n",
    "# run inference: Sample   \n",
    "with model: \n",
    "    \n",
    "    # prior = pm.sample_prior_predictive()\n",
    "    \n",
    "    # obtain starting values via MAP\n",
    "    # startvals = pm.find_MAP()\n",
    "    \n",
    "    trace = pm.sample(draws=1000, \n",
    "                      tune=2000, \n",
    "                      cores=2, \n",
    "                      return_inferencedata=True, \n",
    "                      target_accept=0.97);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace,\n",
    "              figsize=(8,10));\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model: \n",
    "    ppc = pm.sample_posterior_predictive(\n",
    "    trace, var_names=[\"f_s\", \"C\", \"grad_a\", \"f_r\", \"sigma\", \"Y_obs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1, figsize=(6,5),\n",
    "                       sharex=True, sharey=True)\n",
    "\n",
    "for j in [0,1,2]:\n",
    "    sub_df = Elmer_Runs[Elmer_Runs['key_factor']==j]\n",
    "    \n",
    "    for i in np.random.randint(0,999,499):\n",
    "        params = []\n",
    "                \n",
    "        for key in list(ppc)[:-2]: \n",
    "            if len(ppc[key].shape) == 2: \n",
    "                params.append(ppc[key][i, j])\n",
    "            else: \n",
    "                params.append(ppc[key][i])\n",
    "        \n",
    "        ax[j].plot(sub_df.z.values, \n",
    "                   melt_model.eval_forward(sub_df.z.values, *params, parts=False), \n",
    "                   c='tab:blue', \n",
    "                   alpha=0.1)\n",
    "        \n",
    "    ax[j].scatter(sub_df.z, sub_df.MB, c=sub_df.key_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
